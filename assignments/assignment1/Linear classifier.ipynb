{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57611688, 0.21194156, 0.21194156])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax([1, 0, 0])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.551444713932051, array([ 0.57611688, -0.78805844,  0.21194156]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 20653.747661\n",
      "Epoch 1, loss: 20481.261599\n",
      "Epoch 2, loss: 20353.274314\n",
      "Epoch 3, loss: 20263.866344\n",
      "Epoch 4, loss: 20190.995853\n",
      "Epoch 5, loss: 20135.869439\n",
      "Epoch 6, loss: 20097.220545\n",
      "Epoch 7, loss: 20064.299837\n",
      "Epoch 8, loss: 20037.468700\n",
      "Epoch 9, loss: 20021.403196\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=0.0001, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124749940>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.162\n",
      "Epoch 0, loss: 42745.685988\n",
      "Epoch 1, loss: 33313.783635\n",
      "Epoch 2, loss: 30251.175510\n",
      "Epoch 3, loss: 27997.226738\n",
      "Epoch 4, loss: 26383.533250\n",
      "Epoch 5, loss: 25152.938518\n",
      "Epoch 6, loss: 24201.543963\n",
      "Epoch 7, loss: 23529.901845\n",
      "Epoch 8, loss: 22969.178418\n",
      "Epoch 9, loss: 22493.488196\n",
      "Epoch 10, loss: 22106.423898\n",
      "Epoch 11, loss: 21806.097545\n",
      "Epoch 12, loss: 21473.626868\n",
      "Epoch 13, loss: 21244.572821\n",
      "Epoch 14, loss: 21065.463708\n",
      "Epoch 15, loss: 20835.576966\n",
      "Epoch 16, loss: 20621.493012\n",
      "Epoch 17, loss: 20522.087556\n",
      "Epoch 18, loss: 20379.350919\n",
      "Epoch 19, loss: 20254.130850\n",
      "Epoch 20, loss: 20148.835537\n",
      "Epoch 21, loss: 20043.922518\n",
      "Epoch 22, loss: 19961.855432\n",
      "Epoch 23, loss: 19835.023315\n",
      "Epoch 24, loss: 19792.223270\n",
      "Epoch 25, loss: 19724.690670\n",
      "Epoch 26, loss: 19672.199142\n",
      "Epoch 27, loss: 19547.254752\n",
      "Epoch 28, loss: 19531.843601\n",
      "Epoch 29, loss: 19487.299882\n",
      "Epoch 30, loss: 19417.241457\n",
      "Epoch 31, loss: 19375.058801\n",
      "Epoch 32, loss: 19337.595268\n",
      "Epoch 33, loss: 19292.766904\n",
      "Epoch 34, loss: 19354.309895\n",
      "Epoch 35, loss: 19148.933582\n",
      "Epoch 36, loss: 19210.530385\n",
      "Epoch 37, loss: 19159.012011\n",
      "Epoch 38, loss: 19091.620073\n",
      "Epoch 39, loss: 19144.690935\n",
      "Epoch 40, loss: 19044.163737\n",
      "Epoch 41, loss: 19033.645236\n",
      "Epoch 42, loss: 18973.386436\n",
      "Epoch 43, loss: 18966.426176\n",
      "Epoch 44, loss: 18938.217448\n",
      "Epoch 45, loss: 18982.121880\n",
      "Epoch 46, loss: 19006.366364\n",
      "Epoch 47, loss: 18845.760411\n",
      "Epoch 48, loss: 18855.664736\n",
      "Epoch 49, loss: 18838.094689\n",
      "Epoch 50, loss: 18787.382248\n",
      "Epoch 51, loss: 18850.039705\n",
      "Epoch 52, loss: 18892.451008\n",
      "Epoch 53, loss: 18784.189506\n",
      "Epoch 54, loss: 18756.393532\n",
      "Epoch 55, loss: 18749.104652\n",
      "Epoch 56, loss: 18798.818253\n",
      "Epoch 57, loss: 18809.249213\n",
      "Epoch 58, loss: 18771.453832\n",
      "Epoch 59, loss: 18656.168848\n",
      "Epoch 60, loss: 18737.242251\n",
      "Epoch 61, loss: 18713.301116\n",
      "Epoch 62, loss: 18626.633841\n",
      "Epoch 63, loss: 18627.460587\n",
      "Epoch 64, loss: 18645.596905\n",
      "Epoch 65, loss: 18622.115821\n",
      "Epoch 66, loss: 18622.902835\n",
      "Epoch 67, loss: 18643.326166\n",
      "Epoch 68, loss: 18685.771226\n",
      "Epoch 69, loss: 18608.645202\n",
      "Epoch 70, loss: 18589.008792\n",
      "Epoch 71, loss: 18578.120651\n",
      "Epoch 72, loss: 18553.941091\n",
      "Epoch 73, loss: 18619.127170\n",
      "Epoch 74, loss: 18572.278656\n",
      "Epoch 75, loss: 18541.899058\n",
      "Epoch 76, loss: 18541.686739\n",
      "Epoch 77, loss: 18562.980051\n",
      "Epoch 78, loss: 18533.235415\n",
      "Epoch 79, loss: 18508.150669\n",
      "Epoch 80, loss: 18463.883484\n",
      "Epoch 81, loss: 18528.748878\n",
      "Epoch 82, loss: 18498.359728\n",
      "Epoch 83, loss: 18508.718527\n",
      "Epoch 84, loss: 18624.457411\n",
      "Epoch 85, loss: 18565.399044\n",
      "Epoch 86, loss: 18509.035996\n",
      "Epoch 87, loss: 18508.665949\n",
      "Epoch 88, loss: 18534.618507\n",
      "Epoch 89, loss: 18492.721198\n",
      "Epoch 90, loss: 18481.277796\n",
      "Epoch 91, loss: 18467.805086\n",
      "Epoch 92, loss: 18464.247258\n",
      "Epoch 93, loss: 18463.843875\n",
      "Epoch 94, loss: 18548.846016\n",
      "Epoch 95, loss: 18480.472398\n",
      "Epoch 96, loss: 18448.814344\n",
      "Epoch 97, loss: 18559.029097\n",
      "Epoch 98, loss: 18483.251951\n",
      "Epoch 99, loss: 18491.778818\n",
      "Accuracy after training for 100 epochs:  0.246\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=0.0005, batch_size=300, reg=0.1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 20414.744592\n",
      "Epoch 100, loss: 18058.771950\n",
      "Epoch 200, loss: 17637.356112\n",
      "Epoch 300, loss: 17342.296660\n",
      "Epoch 400, loss: 17159.868772\n",
      "Epoch 500, loss: 16990.047392\n",
      "Epoch 600, loss: 16809.909624\n",
      "Epoch 700, loss: 16700.130317\n",
      "Epoch 800, loss: 16580.581054\n",
      "Epoch 900, loss: 16485.270513\n",
      "Epoch 0, loss: 24903.872662\n",
      "Epoch 100, loss: 22297.328380\n",
      "Epoch 200, loss: 20862.942643\n",
      "Epoch 300, loss: 22976.885014\n",
      "Epoch 400, loss: 19402.509755\n",
      "Epoch 500, loss: 21549.557723\n",
      "Epoch 600, loss: 21314.987005\n",
      "Epoch 700, loss: 20249.019410\n",
      "Epoch 800, loss: 19221.430913\n",
      "Epoch 900, loss: 20621.651400\n",
      "Epoch 0, loss: 36192.745230\n",
      "Epoch 100, loss: 37899.599755\n",
      "Epoch 200, loss: 37337.914688\n",
      "Epoch 300, loss: 36141.202273\n",
      "Epoch 400, loss: 36225.974319\n",
      "Epoch 500, loss: 36039.566748\n",
      "Epoch 600, loss: 37541.510030\n",
      "Epoch 700, loss: 33917.702554\n",
      "Epoch 800, loss: 35839.291069\n",
      "Epoch 900, loss: 33170.632683\n",
      "Epoch 0, loss: 75158.276729\n",
      "Epoch 100, loss: 69685.640637\n",
      "Epoch 200, loss: 90978.532976\n",
      "Epoch 300, loss: 80852.115678\n",
      "Epoch 400, loss: 77666.130407\n",
      "Epoch 500, loss: 67046.164446\n",
      "Epoch 600, loss: 70253.995190\n",
      "Epoch 700, loss: 67091.417010\n",
      "Epoch 800, loss: 72596.227297\n",
      "Epoch 900, loss: 81192.749074\n",
      "Epoch 0, loss: 20457.058112\n",
      "Epoch 100, loss: 18076.511612\n",
      "Epoch 200, loss: 17637.516611\n",
      "Epoch 300, loss: 17321.189910\n",
      "Epoch 400, loss: 17114.213867\n",
      "Epoch 500, loss: 16960.251598\n",
      "Epoch 600, loss: 16861.228276\n",
      "Epoch 700, loss: 16684.217309\n",
      "Epoch 800, loss: 16584.866161\n",
      "Epoch 900, loss: 16482.367128\n",
      "Epoch 0, loss: 25123.988474\n",
      "Epoch 100, loss: 21388.139546\n",
      "Epoch 200, loss: 22510.702671\n",
      "Epoch 300, loss: 22064.592137\n",
      "Epoch 400, loss: 20990.448120\n",
      "Epoch 500, loss: 20692.511702\n",
      "Epoch 600, loss: 21092.229166\n",
      "Epoch 700, loss: 21575.610921\n",
      "Epoch 800, loss: 19584.431785\n",
      "Epoch 900, loss: 20314.795519\n",
      "Epoch 0, loss: 41013.672293\n",
      "Epoch 100, loss: 40610.936209\n",
      "Epoch 200, loss: 39560.312121\n",
      "Epoch 300, loss: 37754.591307\n",
      "Epoch 400, loss: 32734.992471\n",
      "Epoch 500, loss: 35570.672517\n",
      "Epoch 600, loss: 34905.670488\n",
      "Epoch 700, loss: 32449.821103\n",
      "Epoch 800, loss: 32575.238773\n",
      "Epoch 900, loss: 32107.384913\n",
      "Epoch 0, loss: 79199.168250\n",
      "Epoch 100, loss: 76605.024927\n",
      "Epoch 200, loss: 80038.008943\n",
      "Epoch 300, loss: 73411.495808\n",
      "Epoch 400, loss: 60118.148257\n",
      "Epoch 500, loss: 79783.949043\n",
      "Epoch 600, loss: 67166.413759\n",
      "Epoch 700, loss: 63931.133361\n",
      "Epoch 800, loss: 54262.201259\n",
      "Epoch 900, loss: 77395.455976\n",
      "Epoch 0, loss: 20432.196851\n",
      "Epoch 100, loss: 18056.565937\n",
      "Epoch 200, loss: 17609.931844\n",
      "Epoch 300, loss: 17378.425490\n",
      "Epoch 400, loss: 17107.681039\n",
      "Epoch 500, loss: 16951.827449\n",
      "Epoch 600, loss: 16846.877954\n",
      "Epoch 700, loss: 16665.467405\n",
      "Epoch 800, loss: 16577.912301\n",
      "Epoch 900, loss: 16459.490592\n",
      "Epoch 0, loss: 25115.657560\n",
      "Epoch 100, loss: 22685.788748\n",
      "Epoch 200, loss: 21902.600530\n",
      "Epoch 300, loss: 19744.560237\n",
      "Epoch 400, loss: 21879.640766\n",
      "Epoch 500, loss: 20118.376900\n",
      "Epoch 600, loss: 20794.715284\n",
      "Epoch 700, loss: 20173.707310\n",
      "Epoch 800, loss: 20995.042445\n",
      "Epoch 900, loss: 20034.103741\n",
      "Epoch 0, loss: 37361.314782\n",
      "Epoch 100, loss: 39841.752437\n",
      "Epoch 200, loss: 39851.939610\n",
      "Epoch 300, loss: 36781.500631\n",
      "Epoch 400, loss: 36866.047585\n",
      "Epoch 500, loss: 32416.368444\n",
      "Epoch 600, loss: 33864.042909\n",
      "Epoch 700, loss: 37030.212549\n",
      "Epoch 800, loss: 33609.732928\n",
      "Epoch 900, loss: 31551.137693\n",
      "Epoch 0, loss: 73689.870370\n",
      "Epoch 100, loss: 69640.051847\n",
      "Epoch 200, loss: 79270.386668\n",
      "Epoch 300, loss: 61211.745779\n",
      "Epoch 400, loss: 78289.981663\n",
      "Epoch 500, loss: 81238.743485\n",
      "Epoch 600, loss: 61688.080125\n",
      "Epoch 700, loss: 59011.883866\n",
      "Epoch 800, loss: 54066.433674\n",
      "Epoch 900, loss: 66405.354296\n",
      "Epoch 0, loss: 20641.007881\n",
      "Epoch 100, loss: 18777.299940\n",
      "Epoch 200, loss: 18515.337216\n",
      "Epoch 300, loss: 18345.899261\n",
      "Epoch 400, loss: 18215.697867\n",
      "Epoch 500, loss: 18105.864186\n",
      "Epoch 600, loss: 18017.351538\n",
      "Epoch 700, loss: 17936.198755\n",
      "Epoch 800, loss: 17867.921830\n",
      "Epoch 900, loss: 17798.529905\n",
      "Epoch 0, loss: 20652.010930\n",
      "Epoch 100, loss: 18776.858971\n",
      "Epoch 200, loss: 18515.688822\n",
      "Epoch 300, loss: 18346.902280\n",
      "Epoch 400, loss: 18217.316564\n",
      "Epoch 500, loss: 18106.594819\n",
      "Epoch 600, loss: 18021.971078\n",
      "Epoch 700, loss: 17936.958982\n",
      "Epoch 800, loss: 17869.094347\n",
      "Epoch 900, loss: 17799.798488\n",
      "Epoch 0, loss: 20649.743034\n",
      "Epoch 100, loss: 18777.139738\n",
      "Epoch 200, loss: 18516.771781\n",
      "Epoch 300, loss: 18355.633266\n",
      "Epoch 400, loss: 18216.957644\n",
      "Epoch 500, loss: 18114.378640\n",
      "Epoch 600, loss: 18018.539880\n",
      "Epoch 700, loss: 17935.996242\n",
      "Epoch 800, loss: 17870.863916\n",
      "Epoch 900, loss: 17796.231730\n",
      "Epoch 0, loss: 20662.694272\n",
      "Epoch 100, loss: 18778.766545\n",
      "Epoch 200, loss: 18528.347167\n",
      "Epoch 300, loss: 18344.367473\n",
      "Epoch 400, loss: 18221.026701\n",
      "Epoch 500, loss: 18109.858633\n",
      "Epoch 600, loss: 18012.722327\n",
      "Epoch 700, loss: 17939.116367\n",
      "Epoch 800, loss: 17862.354449\n",
      "Epoch 900, loss: 17806.099320\n",
      "Epoch 0, loss: 20640.273833\n",
      "Epoch 100, loss: 18776.350452\n",
      "Epoch 200, loss: 18515.051001\n",
      "Epoch 300, loss: 18345.715989\n",
      "Epoch 400, loss: 18213.397186\n",
      "Epoch 500, loss: 18103.774557\n",
      "Epoch 600, loss: 18015.443254\n",
      "Epoch 700, loss: 17937.229674\n",
      "Epoch 800, loss: 17860.809999\n",
      "Epoch 900, loss: 17796.631476\n",
      "Epoch 0, loss: 20647.315612\n",
      "Epoch 100, loss: 18772.426235\n",
      "Epoch 200, loss: 18523.748315\n",
      "Epoch 300, loss: 18345.376687\n",
      "Epoch 400, loss: 18217.589687\n",
      "Epoch 500, loss: 18108.084040\n",
      "Epoch 600, loss: 18014.505399\n",
      "Epoch 700, loss: 17938.863071\n",
      "Epoch 800, loss: 17866.373730\n",
      "Epoch 900, loss: 17800.865823\n",
      "Epoch 0, loss: 20649.002059\n",
      "Epoch 100, loss: 18773.859218\n",
      "Epoch 200, loss: 18523.071386\n",
      "Epoch 300, loss: 18346.628257\n",
      "Epoch 400, loss: 18210.869826\n",
      "Epoch 500, loss: 18116.433270\n",
      "Epoch 600, loss: 18020.187368\n",
      "Epoch 700, loss: 17935.340759\n",
      "Epoch 800, loss: 17860.676722\n",
      "Epoch 900, loss: 17802.095785\n",
      "Epoch 0, loss: 20648.486255\n",
      "Epoch 100, loss: 18782.873347\n",
      "Epoch 200, loss: 18527.668287\n",
      "Epoch 300, loss: 18360.719584\n",
      "Epoch 400, loss: 18222.075474\n",
      "Epoch 500, loss: 18110.313239\n",
      "Epoch 600, loss: 18029.186038\n",
      "Epoch 700, loss: 17942.995716\n",
      "Epoch 800, loss: 17869.573848\n",
      "Epoch 900, loss: 17802.738934\n",
      "Epoch 0, loss: 20638.611826\n",
      "Epoch 100, loss: 18775.982025\n",
      "Epoch 200, loss: 18515.237971\n",
      "Epoch 300, loss: 18347.599782\n",
      "Epoch 400, loss: 18210.117448\n",
      "Epoch 500, loss: 18109.407435\n",
      "Epoch 600, loss: 18016.877679\n",
      "Epoch 700, loss: 17933.719483\n",
      "Epoch 800, loss: 17867.475894\n",
      "Epoch 900, loss: 17797.926406\n",
      "Epoch 0, loss: 20642.615228\n",
      "Epoch 100, loss: 18775.488411\n",
      "Epoch 200, loss: 18512.040736\n",
      "Epoch 300, loss: 18347.068752\n",
      "Epoch 400, loss: 18211.836095\n",
      "Epoch 500, loss: 18106.995067\n",
      "Epoch 600, loss: 18022.957454\n",
      "Epoch 700, loss: 17932.810885\n",
      "Epoch 800, loss: 17859.405579\n",
      "Epoch 900, loss: 17801.693046\n",
      "Epoch 0, loss: 20655.791181\n",
      "Epoch 100, loss: 18777.988398\n",
      "Epoch 200, loss: 18521.696849\n",
      "Epoch 300, loss: 18351.310942\n",
      "Epoch 400, loss: 18220.137619\n",
      "Epoch 500, loss: 18109.165997\n",
      "Epoch 600, loss: 18016.682586\n",
      "Epoch 700, loss: 17946.141594\n",
      "Epoch 800, loss: 17859.676692\n",
      "Epoch 900, loss: 17797.280050\n",
      "Epoch 0, loss: 20663.207483\n",
      "Epoch 100, loss: 18778.429080\n",
      "Epoch 200, loss: 18520.351824\n",
      "Epoch 300, loss: 18357.677623\n",
      "Epoch 400, loss: 18212.989035\n",
      "Epoch 500, loss: 18106.375170\n",
      "Epoch 600, loss: 18031.487787\n",
      "Epoch 700, loss: 17940.361404\n",
      "Epoch 800, loss: 17858.460533\n",
      "Epoch 900, loss: 17802.470737\n",
      "Epoch 0, loss: 20709.577728\n",
      "Epoch 100, loss: 19622.112447\n",
      "Epoch 200, loss: 19328.992879\n",
      "Epoch 300, loss: 19176.704042\n",
      "Epoch 400, loss: 19072.336321\n",
      "Epoch 500, loss: 18992.714483\n",
      "Epoch 600, loss: 18927.892486\n",
      "Epoch 700, loss: 18873.493786\n",
      "Epoch 800, loss: 18826.316846\n",
      "Epoch 900, loss: 18784.323068\n",
      "Epoch 0, loss: 20714.482172\n",
      "Epoch 100, loss: 19623.665390\n",
      "Epoch 200, loss: 19330.208941\n",
      "Epoch 300, loss: 19176.936750\n",
      "Epoch 400, loss: 19072.690034\n",
      "Epoch 500, loss: 18992.775937\n",
      "Epoch 600, loss: 18927.763644\n",
      "Epoch 700, loss: 18873.226158\n",
      "Epoch 800, loss: 18826.161799\n",
      "Epoch 900, loss: 18784.494832\n",
      "Epoch 0, loss: 20714.371204\n",
      "Epoch 100, loss: 19623.593247\n",
      "Epoch 200, loss: 19329.531360\n",
      "Epoch 300, loss: 19177.226892\n",
      "Epoch 400, loss: 19072.498201\n",
      "Epoch 500, loss: 18992.767567\n",
      "Epoch 600, loss: 18928.438411\n",
      "Epoch 700, loss: 18874.130572\n",
      "Epoch 800, loss: 18826.207090\n",
      "Epoch 900, loss: 18784.514261\n",
      "Epoch 0, loss: 20715.366048\n",
      "Epoch 100, loss: 19622.909863\n",
      "Epoch 200, loss: 19329.227832\n",
      "Epoch 300, loss: 19177.164023\n",
      "Epoch 400, loss: 19073.351056\n",
      "Epoch 500, loss: 18992.397459\n",
      "Epoch 600, loss: 18928.112445\n",
      "Epoch 700, loss: 18873.780941\n",
      "Epoch 800, loss: 18825.581320\n",
      "Epoch 900, loss: 18783.775389\n",
      "Epoch 0, loss: 20716.374542\n",
      "Epoch 100, loss: 19623.876170\n",
      "Epoch 200, loss: 19329.992172\n",
      "Epoch 300, loss: 19176.742293\n",
      "Epoch 400, loss: 19072.337853\n",
      "Epoch 500, loss: 18992.615484\n",
      "Epoch 600, loss: 18928.209792\n",
      "Epoch 700, loss: 18873.515712\n",
      "Epoch 800, loss: 18826.316425\n",
      "Epoch 900, loss: 18784.204204\n",
      "Epoch 0, loss: 20712.222935\n",
      "Epoch 100, loss: 19623.639334\n",
      "Epoch 200, loss: 19329.783057\n",
      "Epoch 300, loss: 19176.887964\n",
      "Epoch 400, loss: 19072.185472\n",
      "Epoch 500, loss: 18992.577033\n",
      "Epoch 600, loss: 18927.856756\n",
      "Epoch 700, loss: 18873.310113\n",
      "Epoch 800, loss: 18825.918802\n",
      "Epoch 900, loss: 18784.394216\n",
      "Epoch 0, loss: 20715.675478\n",
      "Epoch 100, loss: 19623.942811\n",
      "Epoch 200, loss: 19329.632063\n",
      "Epoch 300, loss: 19177.267395\n",
      "Epoch 400, loss: 19072.772324\n",
      "Epoch 500, loss: 18992.825817\n",
      "Epoch 600, loss: 18928.120049\n",
      "Epoch 700, loss: 18873.725034\n",
      "Epoch 800, loss: 18826.260135\n",
      "Epoch 900, loss: 18784.596480\n",
      "Epoch 0, loss: 20715.273909\n",
      "Epoch 100, loss: 19623.984088\n",
      "Epoch 200, loss: 19328.890897\n",
      "Epoch 300, loss: 19177.051321\n",
      "Epoch 400, loss: 19072.276802\n",
      "Epoch 500, loss: 18992.947742\n",
      "Epoch 600, loss: 18927.245486\n",
      "Epoch 700, loss: 18873.149452\n",
      "Epoch 800, loss: 18826.688953\n",
      "Epoch 900, loss: 18783.998437\n",
      "Epoch 0, loss: 20714.380848\n",
      "Epoch 100, loss: 19623.398525\n",
      "Epoch 200, loss: 19329.617900\n",
      "Epoch 300, loss: 19176.840361\n",
      "Epoch 400, loss: 19072.436416\n",
      "Epoch 500, loss: 18992.757453\n",
      "Epoch 600, loss: 18927.773656\n",
      "Epoch 700, loss: 18873.324899\n",
      "Epoch 800, loss: 18826.333678\n",
      "Epoch 900, loss: 18784.195731\n",
      "Epoch 0, loss: 20711.591349\n",
      "Epoch 100, loss: 19622.698146\n",
      "Epoch 200, loss: 19329.499420\n",
      "Epoch 300, loss: 19177.195114\n",
      "Epoch 400, loss: 19071.931302\n",
      "Epoch 500, loss: 18992.603848\n",
      "Epoch 600, loss: 18927.939499\n",
      "Epoch 700, loss: 18873.973653\n",
      "Epoch 800, loss: 18825.931673\n",
      "Epoch 900, loss: 18784.499458\n",
      "Epoch 0, loss: 20717.418241\n",
      "Epoch 100, loss: 19623.788606\n",
      "Epoch 200, loss: 19329.768858\n",
      "Epoch 300, loss: 19177.116051\n",
      "Epoch 400, loss: 19072.522214\n",
      "Epoch 500, loss: 18992.729912\n",
      "Epoch 600, loss: 18927.484986\n",
      "Epoch 700, loss: 18873.369367\n",
      "Epoch 800, loss: 18826.702838\n",
      "Epoch 900, loss: 18784.097146\n",
      "Epoch 0, loss: 20715.503114\n",
      "Epoch 100, loss: 19623.751079\n",
      "Epoch 200, loss: 19329.551663\n",
      "Epoch 300, loss: 19176.956039\n",
      "Epoch 400, loss: 19072.531191\n",
      "Epoch 500, loss: 18992.655207\n",
      "Epoch 600, loss: 18928.030332\n",
      "Epoch 700, loss: 18873.176337\n",
      "Epoch 800, loss: 18825.775050\n",
      "Epoch 900, loss: 18783.800653\n",
      "best validation accuracy achieved: 0.251000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "learning_rates = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "batch_size = [100, 300, 500, 1000]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        for bs in batch_size:\n",
    "            classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "            classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=bs, reg=rs)\n",
    "            pred = classifier.predict(val_X)\n",
    "            accuracy = multiclass_accuracy(pred, val_y)\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.204000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
