{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25027059, -0.25017417, -0.25009492,  0.        ],\n",
       "       [-0.50054119,  0.50034834,  0.50018983,  0.        ],\n",
       "       [ 0.75081178,  0.02501742,  0.02500949,  0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl = layers.FullyConnectedLayer(3, 4)\n",
    "rl = layers.ReLULayer()\n",
    "\n",
    "fc_out = fcl.forward(X)\n",
    "rl_out = rl.forward(fc_out)\n",
    "loss, grad = layers.softmax_with_cross_entropy(rl_out, np.array([[2], [3]]))\n",
    "\n",
    "rl_grad = rl.backward(grad)\n",
    "fcl.backward(rl_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 48.604311, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 48.137702, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.719709, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.499006, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.148582, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.087704, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.625874, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.425360, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.458473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.517355, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.321953, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.345858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.921689, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.810810, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.909649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.675756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.368042, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.906198, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.010345, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.577869, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-5)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b3a1c50>]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 42.533813, Train accuracy: 0.307667, val accuracy: 0.303000\n",
      "Loss: 29.044808, Train accuracy: 0.444778, val accuracy: 0.448000\n",
      "Loss: 30.694454, Train accuracy: 0.466667, val accuracy: 0.491000\n",
      "Loss: 35.612815, Train accuracy: 0.527222, val accuracy: 0.543000\n",
      "Loss: 31.095008, Train accuracy: 0.582667, val accuracy: 0.574000\n",
      "Loss: 36.012084, Train accuracy: 0.565333, val accuracy: 0.572000\n",
      "Loss: 35.302729, Train accuracy: 0.551778, val accuracy: 0.533000\n",
      "Loss: 30.093066, Train accuracy: 0.584000, val accuracy: 0.578000\n",
      "Loss: 26.530618, Train accuracy: 0.622889, val accuracy: 0.593000\n",
      "Loss: 38.297471, Train accuracy: 0.583333, val accuracy: 0.571000\n",
      "Loss: 32.631292, Train accuracy: 0.617778, val accuracy: 0.609000\n",
      "Loss: 34.043517, Train accuracy: 0.621000, val accuracy: 0.595000\n",
      "Loss: 34.287042, Train accuracy: 0.585778, val accuracy: 0.601000\n",
      "Loss: 42.360692, Train accuracy: 0.603111, val accuracy: 0.613000\n",
      "Loss: 42.656437, Train accuracy: 0.618333, val accuracy: 0.585000\n",
      "Loss: 31.131178, Train accuracy: 0.603889, val accuracy: 0.593000\n",
      "Loss: 32.741411, Train accuracy: 0.619111, val accuracy: 0.585000\n",
      "Loss: 36.022471, Train accuracy: 0.539667, val accuracy: 0.543000\n",
      "Loss: 35.339967, Train accuracy: 0.632444, val accuracy: 0.624000\n",
      "Loss: 35.901787, Train accuracy: 0.646556, val accuracy: 0.627000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 45.478225, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.667138, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.390830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.670050, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.882877, Train accuracy: 0.202667, val accuracy: 0.208000\n",
      "Loss: 41.527208, Train accuracy: 0.243000, val accuracy: 0.244000\n",
      "Loss: 44.829499, Train accuracy: 0.273556, val accuracy: 0.268000\n",
      "Loss: 39.459344, Train accuracy: 0.285556, val accuracy: 0.287000\n",
      "Loss: 39.931152, Train accuracy: 0.330000, val accuracy: 0.331000\n",
      "Loss: 38.959392, Train accuracy: 0.384667, val accuracy: 0.379000\n",
      "Loss: 33.765038, Train accuracy: 0.429111, val accuracy: 0.417000\n",
      "Loss: 38.809729, Train accuracy: 0.464222, val accuracy: 0.441000\n",
      "Loss: 30.889516, Train accuracy: 0.489667, val accuracy: 0.475000\n",
      "Loss: 32.248849, Train accuracy: 0.516556, val accuracy: 0.509000\n",
      "Loss: 33.604396, Train accuracy: 0.542444, val accuracy: 0.531000\n",
      "Loss: 26.999726, Train accuracy: 0.566111, val accuracy: 0.556000\n",
      "Loss: 31.355946, Train accuracy: 0.589667, val accuracy: 0.579000\n",
      "Loss: 28.601876, Train accuracy: 0.586667, val accuracy: 0.587000\n",
      "Loss: 30.161980, Train accuracy: 0.610889, val accuracy: 0.611000\n",
      "Loss: 30.184338, Train accuracy: 0.624778, val accuracy: 0.613000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.563231, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.517835, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.552843, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 11.472890, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 11.369080, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 11.442454, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.355635, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.323866, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.432619, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 10.968362, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.309406, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.523223, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.534479, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 10.837883, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 10.981531, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.143398, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.065089, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.228350, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.447900, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 9.975150, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.157850, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 9.289764, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 9.562365, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.295661, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.239246, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.168363, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.550139, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.849927, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 7.256815, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 7.649172, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 7.573674, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.824338, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.118854, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.091958, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 7.356654, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 8.831272, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 7.451411, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 7.626092, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 8.416826, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 9.786595, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 5.434732, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 8.100741, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 7.968020, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 8.346364, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 6.649143, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 7.475458, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 7.019941, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 6.452553, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 5.845562, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 7.200114, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 6.873772, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 8.003645, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 7.210362, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 3.745228, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 7.563326, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 3.848802, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 6.835717, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.961544, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 3.349074, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.225786, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.359413, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 2.252781, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 3.429702, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.223947, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 4.421149, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 3.926157, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 4.225522, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.430605, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 8.297405, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 4.652961, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 4.800398, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 6.056023, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 4.512402, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.876974, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.111662, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 3.602366, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.582589, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 4.612066, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 3.789592, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.817681, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 4.889882, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 5.724410, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 3.702279, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.665950, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 2.408125, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.455138, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.242878, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.322691, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.032223, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.860527, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 3.326582, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 4.060203, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 3.991276, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.413840, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.728986, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.577572, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.266897, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.162457, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.566957, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.441526, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.351084, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.251928, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.290603, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.822760, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.720850, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.960715, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.433421, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.602133, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.735460, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.327920, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.718499, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.336395, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.807828, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.487411, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.033013, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.577722, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.329011, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.650434, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.121378, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.660394, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.898594, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.872599, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.662763, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.969546, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.909976, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.272559, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.696504, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.731622, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.782605, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.095466, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.082771, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.802384, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.988420, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.775887, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.048783, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.030491, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.034552, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.780690, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.700613, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.759707, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.972260, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.980218, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.686002, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.925167, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.819837, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.774826, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.814258, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.882128, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.733875, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.732052, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.01, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.579431, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.460335, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.526839, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.680437, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.029123, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 10.142371, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 8.809558, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 7.413398, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.256147, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 7.453836, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: 7.189049, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 8.755959, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 6.346957, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 5.559525, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 3.954795, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 5.039667, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 3.710749, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 5.277621, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.016605, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.471122, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=0.01, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################### 0.0001 - 0.001 - 256 - 8 ################################\n",
      "Loss: 16.119796, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 15.278978, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 11.720001, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 256 - 16 ################################\n",
      "Loss: 34.540260, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: 33.670724, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 27.610002, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 256 - 32 ################################\n",
      "Loss: 34.541772, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 33.616333, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 27.316893, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 256 - 64 ################################\n",
      "Loss: 34.534939, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 33.607814, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 27.010798, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 1024 - 8 ################################\n",
      "Loss: 16.120503, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 15.181778, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 11.365000, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 1024 - 16 ################################\n",
      "Loss: 34.537423, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 32.947508, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 24.321197, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 1024 - 32 ################################\n",
      "Loss: 34.545340, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: 32.927792, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 24.337613, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 1024 - 64 ################################\n",
      "Loss: 34.547472, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 32.864957, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 24.330381, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 4096 - 8 ################################\n",
      "Loss: 16.127013, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 14.391060, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 10.428160, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "############################### 0.0001 - 0.001 - 4096 - 16 ################################\n",
      "Loss: 34.556923, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: 30.787309, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 22.138910, Train accuracy: 0.400000, val accuracy: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-9417c74eec71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlcourse_ai/assignments/assignment2/optim.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, w, d_w, learning_rate)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \"\"\"\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvelocity\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "reg_strength = [1e-3, 1e-1, 10, 100]\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = [256, 1024, 4096]\n",
    "num_epochs = 300\n",
    "batch_size = [8, 16, 32, 64]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "\n",
    "for le in learning_rates:\n",
    "    for rs in reg_strength:\n",
    "        for hls in hidden_layer_size:\n",
    "            for bs in batch_size:\n",
    "                print('############################### {} - {} - {} - {} ################################'.format(le, rs, hls, bs))\n",
    "                model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hls, reg = rs)\n",
    "                dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "                trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=le, num_epochs=num_epochs, batch_size=bs)\n",
    "\n",
    "                loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "                if max(val_history) > best_val_accuracy:\n",
    "                    best_val_accuracy = max(val_history)\n",
    "                    best_classifier = model\n",
    "    \n",
    "    \n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x117a1cc18>]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-83c278478367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Neural net test set accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
