{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25027059, -0.25017417, -0.25009492,  0.        ],\n",
       "       [-0.50054119,  0.50034834,  0.50018983,  0.        ],\n",
       "       [ 0.75081178,  0.02501742,  0.02500949,  0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl = layers.FullyConnectedLayer(3, 4)\n",
    "rl = layers.ReLULayer()\n",
    "\n",
    "fc_out = fcl.forward(X)\n",
    "rl_out = rl.forward(fc_out)\n",
    "loss, grad = layers.softmax_with_cross_entropy(rl_out, np.array([[2], [3]]))\n",
    "\n",
    "rl_grad = rl.backward(grad)\n",
    "fcl.backward(rl_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 49.131128, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.933072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 47.230118, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.790309, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.311753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.313882, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.201101, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.787037, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.897894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.451351, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.108057, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.832928, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.838535, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.732607, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.747395, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.938610, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.597857, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.946466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.007212, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 44.717487, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x118d17c18>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 46.085567, Train accuracy: 0.076444, val accuracy: 0.078000\n",
      "Loss: 46.064083, Train accuracy: 0.093111, val accuracy: 0.090000\n",
      "Loss: 45.876440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.801908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.888154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.704522, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.466663, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.534060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.782353, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.577125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.816542, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 44.474539, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.612995, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.514273, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.825244, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.702812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.858818, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.363262, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.257144, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.185399, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 46.081841, Train accuracy: 0.064667, val accuracy: 0.065000\n",
      "Loss: 46.087186, Train accuracy: 0.073778, val accuracy: 0.066000\n",
      "Loss: 46.076610, Train accuracy: 0.172556, val accuracy: 0.168000\n",
      "Loss: 46.078608, Train accuracy: 0.194222, val accuracy: 0.196000\n",
      "Loss: 46.079637, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.074634, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.067347, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.065350, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.048162, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.080342, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.060049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.073899, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.075699, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.062076, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.930889, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.018302, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.061634, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.024169, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.026503, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.948227, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.543035, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 11.704782, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.456361, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.401492, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: 11.391362, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 11.261900, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 11.035712, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 11.949688, Train accuracy: 0.333333, val accuracy: 0.200000\n",
      "Loss: 10.944644, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 11.372398, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 9.168142, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 12.155777, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.495186, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 8.560877, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 13.519318, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: 24.630881, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 41.821042, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 15.363985, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: 13.019903, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 10.699444, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 57.580275, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 37.209701, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 18.184078, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 11.457357, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 50.617509, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 68.077343, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: 27.366646, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 201.255588, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: 66.665691, Train accuracy: 0.466667, val accuracy: 0.200000\n",
      "Loss: 42.994040, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: 101.303310, Train accuracy: 0.200000, val accuracy: 0.333333\n",
      "Loss: 79.470343, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 551.562170, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: 207.988892, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: 802.291618, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: 316.081606, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 697.465003, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 81.275081, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 914.063442, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: 741.156861, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
